# -*- coding: utf-8 -*-
"""aiinterview_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fNYNc0lVW5avHwdNohVRNj74NFJl2six

**This project builds an AI-powered interview preparation system using Large Language Models (LLMs) to simulate real technical interviews. The platform extracts relevant skills from a candidateâ€™s resume and generates role-specific interview questions with increasing difficulty. It also evaluates candidate answers by providing structured feedback, including scores, strengths, weaknesses, and improvement suggestions. The system is designed with a modular, API-ready backend architecture and demonstrated using a Gradio-based interface for interactive usage. This project showcases practical applications of NLP, prompt engineering, and LLM-based evaluation in real-world hiring and interview preparation scenarios**
"""

!pip install --upgrade openai spacy

!python -m spacy download en_core_web_sm

import os
os.environ["OPENAI_API_KEY"] = "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"

import spacy
from openai import OpenAI

nlp = spacy.load("en_core_web_sm")

resume_text = """
I am a computer science student skilled in Python,
Data Structures, Machine Learning, SQL, and NLP.
"""

job_role = "Software Development Engineer Intern"

skills_db = [
    "python", "java", "sql",
    "machine learning", "nlp",
    "data structures"
]

def extract_skills(resume):
    resume = resume.lower()
    return [skill for skill in skills_db if skill in resume]

skills = extract_skills(resume_text)
print("Extracted Skills:", skills)

client = OpenAI()

def generate_questions(skills, role):
    prompt = f"""
You are a technical interviewer.
Job role: {role}
Candidate skills: {skills}

Generate 5 interview questions from easy to hard.
"""
    response = client.responses.create(
        model="gpt-4.1-mini",
        input=prompt
    )
    return response.output_text

questions = generate_questions(skills, job_role)
print("Interview Questions:\n", questions)

user_answer = """
I would use a hash map to store values and
check complements efficiently.
"""

!pip install transformers torch accelerate

from transformers import pipeline

evaluator = pipeline(
    "text-generation",
    model="mistralai/Mistral-7B-Instruct-v0.2",
    device_map="auto"
)

# ===============================
# INTERVIEW QUESTION GENERATOR
# ===============================

def generate_questions(skills, role):
    prompt = f"""
You are a technical interviewer.
Job role: {role}
Candidate skills: {skills}

Generate 5 interview questions from easy to hard.
"""
    result = generator(
        prompt,
        max_new_tokens=250,
        temperature=0.4
    )
    return result[0]["generated_text"]

def evaluate_answer(question, answer):
    prompt = f"""
Question: {question}
Answer: {answer}

Evaluate based on:
- Correctness
- Clarity
- Depth

Give:
- Score out of 10
- Strengths
- Weaknesses
- Improvements
"""
    result = evaluator(
        prompt,
        max_new_tokens=300,
        temperature=0.3
    )
    return result[0]["generated_text"]

# ===============================
# MAIN INTERVIEW PIPELINE
# ===============================

def interview_pipeline(resume_text, job_role, user_answer):
    skills = extract_skills(resume_text)
    questions = generate_questions(skills, job_role)
    evaluation = evaluate_answer(questions, user_answer)

    return {
        "skills": skills,
        "questions": questions,
        "evaluation": evaluation
    }

# ===============================
# MAIN INTERVIEW PIPELINE
# ===============================

def interview_pipeline(resume_text, job_role, user_answer):
    skills = extract_skills(resume_text)
    questions = generate_questions(skills, job_role)
    evaluation = evaluate_answer(questions, user_answer)

    return {
        "skills": skills,
        "questions": questions,
        "evaluation": evaluation
    }

# ===============================
# 1. INSTALL LIBRARIES
# ===============================
!pip install -q transformers torch accelerate

# ===============================
# 2. IMPORT
# ===============================
from transformers import pipeline

# ===============================
# 3. LOAD FAST MODEL (CPU FRIENDLY)
# ===============================
generator = pipeline(
    "text2text-generation",
    model="google/flan-t5-base"
)

evaluator = generator  # reuse same model

# ===============================
# 4. SKILL EXTRACTION
# ===============================
skills_db = [
    "python", "java", "sql",
    "machine learning", "nlp",
    "data structures", "algorithms"
]

def extract_skills(resume_text):
    resume_text = resume_text.lower()
    return [skill for skill in skills_db if skill in resume_text]

# ===============================
# 5. QUESTION GENERATION (FAST)
# ===============================
def generate_questions(skills, role):
    prompt = f"""
Generate 5 technical interview questions (easy to hard)
for the role: {role}
based on skills: {skills}
"""
    result = generator(
        prompt,
        max_length=256
    )
    return result[0]["generated_text"]

# ===============================
# 6. ANSWER EVALUATION (FAST)
# ===============================
def evaluate_answer(question, answer):
    prompt = f"""
Question: {question}
Answer: {answer}

Evaluate the answer and give:
Score /10,
Strengths,
Weaknesses,
Improvements.
"""
    result = evaluator(
        prompt,
        max_length=256
    )
    return result[0]["generated_text"]

# ===============================
# 7. PIPELINE
# ===============================
def interview_pipeline(resume_text, job_role, user_answer):
    skills = extract_skills(resume_text)
    questions = generate_questions(skills, job_role)
    evaluation = evaluate_answer(questions, user_answer)

    return {
        "skills": skills,
        "questions": questions,
        "evaluation": evaluation
    }

# ===============================
# 8. TEST
# ===============================
resume_text = """
I am a computer science student skilled in Python,
SQL, Machine Learning, NLP, and Data Structures.
"""

job_role = "Software Development Engineer Intern"

user_answer = """
I would use a hash map to store visited elements
and find complements in linear time.
"""

result = interview_pipeline(resume_text, job_role, user_answer)

print(result)

"""From here, We are connecting to APIS"""

!pip install -q fastapi uvicorn nest-asyncio pyngrok transformers torch accelerate

from fastapi import FastAPI
from pydantic import BaseModel
from transformers import pipeline
import nest_asyncio

generator = pipeline(
    "text-generation",
    model="mistralai/Mistral-7B-Instruct-v0.2",
    device_map="auto"
)

evaluator = generator

skills_db = [
    "python", "java", "sql",
    "machine learning", "nlp",
    "data structures", "algorithms"
]

def extract_skills(resume_text):
    resume_text = resume_text.lower()
    return [skill for skill in skills_db if skill in resume_text]

def generate_questions(skills, role):
    prompt = f"""
You are a technical interviewer.
Job role: {role}
Candidate skills: {skills}

Generate 5 interview questions from easy to hard.
"""
    result = generator(
        prompt,
        max_new_tokens=300,
        temperature=0.4
    )
    return result[0]["generated_text"]

def evaluate_answer(question, answer):
    prompt = f"""
Question: {question}

Candidate Answer:
{answer}

Evaluate based on:
- Correctness
- Clarity
- Depth

Give:
- Score out of 10
- Strengths
- Weaknesses
- Improvements
"""
    result = evaluator(
        prompt,
        max_new_tokens=300,
        temperature=0.3
    )
    return result[0]["generated_text"]

app = FastAPI()

class InterviewRequest(BaseModel):
    resume: str
    role: str
    answer: str

@app.post("/interview")
def interview(req: InterviewRequest):
    skills = extract_skills(req.resume)
    questions = generate_questions(skills, req.role)
    evaluation = evaluate_answer(questions, req.answer)

    return {
        "skills": skills,
        "questions": questions,
        "evaluation": evaluation
    }

!pip install -q gradio

!pip install -q gradio
import gradio as gr

def interview_demo(resume, role, answer):
    skills = extract_skills(resume)
    questions = generate_questions(skills, role)
    evaluation = evaluate_answer(questions, answer)
    return skills, questions, evaluation

gr.Interface(
    fn=interview_demo,
    inputs=[
        gr.Textbox(label="Resume"),
        gr.Textbox(label="Job Role"),
        gr.Textbox(label="Answer")
    ],
    outputs=[
        gr.Textbox(label="Extracted Skills"),
        gr.Textbox(label="Interview Questions"),
        gr.Textbox(label="Evaluation Feedback")
    ],
    title="AI Interview Preparation Platform"
).launch(share=True)